
# Generative AI RAG Pipeline with Pathway and OpenAI API



## Overview
This project showcases the creation of a Generative AI pipeline utilizing Pathway and OpenAI. The application is capable of ingesting and processing data from various sources, including local filesystems, Google Drive, or SharePoint, and responds to user queries via a Retrieval-Augmented Generation (RAG) system. By using Pathway’s connectors, the pipeline continuously monitors data changes, processes unstructured data, and indexes it with embeddings generated by OpenAI’s models.


## Video demo of the project



## Purpose of the project
The objective of this project is to build a scalable question-answering system by leveraging Pathway’s real-time processing capabilities and OpenAI's models for embeddings and language understanding. The application retrieves data from various sources, converts it into structured formats, and allows users to query the data via an HTTP API.

This project illustrates:

-Pathway for real-time data processing and change tracking.
-Langchain for chaining multiple LLM-powered operations (optional).
-LlamaIndex for managing large-scale data retrieval (optional).
-Ollama for model inference (optional).
-OpenAI for embeddings and language model inference.
It emphasizes the integration of these technologies to enable efficient data management and intelligent query processing.

## Features available
-Ingest and index data from various sources, including local files, Google Drive, and SharePoint.
-Query large datasets using OpenAI embeddings within a Retrieval-Augmented Generation (RAG) system.
-Continuously monitor data changes in real-time with low-latency processing.
-Provide an HTTP API for querying the system, delivering intelligent answers from the indexed content.

## Files in the folder
This folder includes the following files:

-app.yaml: Configuration file for data sources, the OpenAI LLM model, and the web server. Customize this file to modify the LLM model, switch to a Google Drive data source, or change filesystem directories for indexing.
-.env: A configuration file where the OpenAI API key is stored as an environment variable.
-Dockerfile: Configuration file for Docker, allowing the pipeline to run within a container.
-requirements.txt: Lists the dependencies for the pipeline. Use pip install -r requirements.txt to install all the necessary packages for running the pipeline locally.
-app.py: The main application code written in Python, utilizing Pathway.
-data/: A folder containing sample files for testing the pipeline.

## Project Steps:
1. Data Ingestion: Pathway connectors ingest files from various sources such as local drives, Google Drive, and SharePoint. These files are read into a unified Pathway Table as binary objects.
2. Content Parsing: The binary objects are parsed using the unstructured library, and the parsed content is then divided into smaller, manageable chunks.
3. Embeddings Generation: These chunks are sent to OpenAI’s API to generate embeddings, which are vector representations of the text.
4. Indexing: The generated embeddings are indexed using Pathway’s machine-learning library, making the data searchable and enabling fast query responses.
5. Querying: Users can retrieve information from the indexed data by making HTTP requests to the defined endpoints. The system supports querying through RESTful API endpoints.

## Prerequisites
- Docker: Ensure Docker is installed on your system for containerizing and running the pipeline.
- OpenAI API Key: You’ll need an OpenAI API key for generating embeddings and performing language model inference.
- Python: Python should be installed locally to manage and install the necessary dependencies for the pipeline.
- VS code
You must have a valid OpenAI key stored in the environment variable OPENAI_API_KEY.


## Running the project
### Verify Docker Installation
Ensure Docker is installed by running the following command:

docker --version
In order to let the pipeline get updated with each change in local files, you need to mount the folder onto the docker. The following commands show how to do that.

#### Make sure you are in the right directory.
cd yourDirectory #where files are there

#### Build the image in this folder
docker build -t rag .

#### Run the image, mount the `data` folder into image and expose the port `8000`
docker run -v `%cd%`/data:/app/data -p 8000:8000 qa

## Query the application
You will see the logs for parsing & embedding documents in the Docker image logs. Give it a few minutes to finish up on embeddings, you will see 0 entries (x minibatch(es)) have been... message. If there are no more updates, this means the app is ready for use!

To test it, let's query the stats:

Invoke-WebRequest -Uri 'http://localhost:8000/v1/pw_list_documents' `
                  -Method POST `
                  -Headers @{ "accept"="/"; "Content-Type"="application/json" } `
                  -Body '{}'

## Asking questions to LLM
Invoke-RestMethod -Method POST `
  -Uri 'http://localhost:8000/v1/pw_ai_answer' `
  -Headers @{ "accept"="/"; "Content-Type"="application/json" } `
  -Body '{"prompt": "What are the key courses?"}'

## Following is the Output of my prompt given to LLM app

![image](https://github.com/user-attachments/assets/32bce3cc-75a7-4643-8cc0-1956e92a883e)


#### Adding Files to Index

First, you can try adding your files and seeing changes in the index. To test index updates, simply add more files to the `data` folder.

If you are using Google Drive or other sources, simply upload your files there.
